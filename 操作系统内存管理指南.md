# 操作系统内存管理
## 什么是物理内存
我们常说的物理内存大小就是指内存条的大小，但是也要看CPU地址总线的位数。

## 使用物理内存的缺点
1. 单个进程直接操作物理地址，可能破坏系统
2. 多进程运行，多个进程同时操作同一地址空间，产生并发读写问题

## 虚拟内存技术
### 虚拟内存
虚拟出来的内存，确保每个进程都有自己的地址空间，一般是4G。操作系统自动将虚拟地址空间映射到物理地址空间，程序关注的只是虚拟内存，请求的也是虚拟内存，但是真正实用的是物理内存。

### 虚拟内存技术特点
* 大的用户空间：32位虚拟地址可以访问4G
* 部分交换
* 连续性：对连续的虚拟地址来映射物理内存中的不连续的大内存缓冲区
* 安全性：不同进程的虚拟地址彼此隔离

### 虚拟内存如何映射到物理内存
CPU中有一个内存管理单元，MMU(Memory Management Unit)，虚拟内存不是直接送到内存总线，而是先给到MMU，由MMU来把虚拟地址映射到物理地址。

MMU通过页表将虚拟地址转换为物理地址，32位的虚拟地址分成两部分（虚拟页号和偏移量），MMU通过页表找到了虚拟页号对应的物理页号，物理页号+偏移量就是实际的物理地址。

页表的目的就是虚拟页面映射为物理内存的页框，页表可以理解为一个数学函数，函数的输入是虚拟页号，函数的输出是物理页号，通过这个函数可以把虚拟页面映射到物理页号，从而确定物理地址。

内存地址转换，三个步骤：
1. 把虚拟内存地址，切分成页号和偏移量
2. 根据页号，从页表里面，查询对应的物理页号
3. 拿物理地址，加上偏移量，得到物理内存地址

### 内存分页
分页是把整个虚拟和物理内存空间切成一段段固定尺寸大小，这个连续并且尺寸固定的内存空间，我们叫页。在linux下，每一页的大小为4KB。

在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表 包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址。这种被称为简单分页。

### 简单分页缺陷
存在空间上的缺陷。

因为操作系统是可以同时运⾏⾮常多的进程的，这就意味着⻚表会⾮常的庞⼤。在32位的环境下，虚拟地址空间共有4GB，假设⼀个⻚的⼤⼩是4KB（2^12），那么就需要⼤约100万 （2^20）个⻚，每个「⻚表项」需要4个字节⼤⼩来存储，那么整个4GB空间的映射就需要有 4MB的内存来存储⻚表。这4MB⼤⼩的⻚表，看起来也不是很⼤。但是要知道每个进程都是有⾃⼰的虚拟地址空间的，也就说都有⾃⼰的⻚表。那么， 100 个进程的话，就需要 400MB 的内存来存储⻚表，这是⾮常⼤的内存了，更别说64位的环境了。

### 多级页表
要解决上⾯的问题，就需要采⽤⼀种叫作多级⻚表（Multi-Level Page Table）的解决⽅案。

对于单页表的实现方式，在32位和页大小4KB的环境下，一个进程的页表需要装下100多万个页表项，并且每个页表项是占用4字节大小，相当于每个页表需占用4MB大小空间。

我们把这个100多万个页表项的单级页表再分页，将页表（一级页表）分为1024个页表（二级页表），每个表（二级页表）中包含1024个页表项，形成二级分页。

分了二级表，映射4GB地址空间需要4KB（一级页表）+4MB（二级页表）的内存，这样占用空间不是更大了吗？

当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，⼆级分⻚占⽤空间确实是更⼤
了，但是，我们往往不会为⼀个进程分配那么多内存。

如果使⽤了⼆级分⻚，⼀级⻚表就可以覆盖整个4GB虚拟地址空间，但如果某个⼀级⻚表的
⻚表项没有被⽤到，也就不需要创建这个⻚表项对应的⼆级⻚表了，即可以在需要时才创建
⼆级⻚表。所以⻚表⼀定要覆盖全部虚拟地址空间，不分级的⻚表就需要有100多万个⻚表项来映射，⽽⼆级分⻚则只需要1024个⻚表项（此时⼀级⻚表覆盖到了全部虚拟地址空间，⼆级⻚表在需要时创建）。

我们把⼆级分⻚再推⼴到多级⻚表，就会发现⻚表占⽤的内存空间更少了，这⼀切都要归功
于对局部性原理的充分应⽤。

对于64位的系统，两级分页肯定不够了，就变成了四级目录，分别是：
* 全局⻚⽬录项 PGD（Page Global Directory）
* 上层⻚⽬录项 PUD（Page Upper Directory）
* 中间⻚⽬录项 PMD（Page Middle Directory）
* ⻚表项 PTE（Page Table Entry）

### TLB
多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这两地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在⼀段时间内，整个程序的执⾏仅限于程序中的某⼀部分。相应地，
执⾏所访问的存储空间也局限于某个内存区域。

我们就可以利⽤这⼀特性，把最常访问的⼏个⻚表项存储到访问速度更快的硬件，于是计算
机科学家们，就在 CPU 芯⽚中，加⼊了⼀个专⻔存放程序最常访问的⻚表项的Cache，这个
Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为⻚表缓存、转址旁路缓存、快表等。

在 CPU 芯⽚⾥⾯，封装了内存管理单元（Memory Management Unit）芯⽚，它⽤来完成地
址转换和 TLB 的访问与交互。有了TLB后，那么CPU在寻址时，会先查TLB，如果没找到，才会继续查常规的⻚表。TLB 的命中率其实是很⾼的，因为程序最常访问的⻚就那么⼏个。


## CPU CACHE

### CPU CACHE分类
cpu高速缓存，处理速度比寄存器慢了一点，cpu cache分为L1 L2 L3三层，L1分成数据缓存和指令缓存，L1离cpu最近，速度最快。
如果把cpu当做大脑，寄存器就是你正在思考的事情，L1就是短期记忆，L2 L3就是长期记忆。

L1 cache通常分为数据缓存（dcache）和指令缓存（icache）。
L3通常要比L1 和L2 cache大很多，是因为L1和L2都是每个CPU核心独有，但是L3是多个cpu核心共享的。

程序执行时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核⼼独有的 L2Cache，最后进⼊到最快的 L1 Cache，之后才会被 CPU 读取。
越靠近cpu核心，访问速度越快。

### 写出让cpu跑的更快地代码 = 写出让cpu缓存命中率高的代码
* 提升数据缓存的命中率 ->代码中最好申请和访问内存连续分布
* 提升指令缓存的命中率 ->先排序后遍历 比 先遍历后排序要快，是因为CPU本身的分支预测器机制。
如果一个进程在不同核心之间切换，各个核心的缓存命中率就会受到影响。所以这也是进程和线程为啥要绑核的原因，就是为了提高cpu缓存命中率。

### cpu缓存一致性
cache和内存的一致性，分为写直达和写回
写直达：把数据同时写入内存和cache，每次都要做判断，性能受到影响。
写回：发生写操作时，新的数据被写到cache block中，只有被修改过的cache block才会写回到内存。

### 缓存一致性问题
* 写传播：某个cpu核心里的cache数据更新时，必须传播到其他核心的cache
* 事务的串行化：某个cpu核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的
* 总线嗅探机制：某个cpu核心的cache更新数据这个事件 能被其他cpu核心知道，但是不保证事务串行化。于是有了MESI协议，这个协议做到了cpu缓存一致性。

总结：
CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读
写性能相⽐内存⾼出很多。对于 Cache ⾥没有缓存 CPU 所需要读取的数据的这种情况，
CPU 则会从内存读取数据，并将数据缓存到 Cache ⾥⾯，最后 CPU 再从 Cache 读取数
据。

⽽对于数据的写⼊， CPU 都会先写⼊到 Cache ⾥⾯，然后再在找个合适的时机写⼊到内存，
那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据⼀致性：
写直达，只要有数据写⼊，都会直接把数据写⼊到内存⾥⾯，这种⽅式简单直观，但是性
能就会受限于内存的访问速度；

写回，对于已经缓存在 Cache 的数据的写⼊，只需要更新其数据就可以，不⽤写⼊到内
存，只有在需要把缓存⾥⾯的脏数据交换出去的时候，才把数据同步到内存⾥，这种⽅式
在缓存命中率⾼的情况，性能会更好；

当今 CPU 都是多核的，每个核⼼都有各⾃独⽴的 L1/L2 Cache，只有 L3 Cache 是多个核⼼
之间共享的。所以，我们要确保多核缓存是⼀致性的，否则会出现错误的结果。
要想实现缓存⼀致性，关键是要满⾜ 2 点：
* 第⼀点是写传播，也就是当某个 CPU 核⼼发⽣写⼊操作时，需要把该事件⼴播通知给其
他核⼼；
* 第⼆点是事物的串⾏化，这个很重要，只有保证了这个，才能保障我们的数据是真正⼀致
的，我们的程序在各个不同的核⼼上运⾏的结果也是⼀致的；

基于总线嗅探机制的 MESI 协议，就满⾜上⾯了这两点，因此它是保障缓存⼀致性的协议。
MESI 协议，是已修改、独占、共享、已实现这四个状态的英⽂缩写的组合。整个 MSI 状态
的变更，则是根据来⾃本地 CPU 核⼼的请求，或者来⾃其他 CPU 核⼼通过总线传输过来的
请求，从⽽构成⼀个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache
Line，修改更新其数据不需要发送⼴播给其他 CPU 核⼼。


